---
title: "On Sparse Modern Hopfield Model"
collection: publications
permalink: /publication/2009-10-01-paper-title-number-1
excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2009-10-01
venue: 'Journal 1'
paperurl: 'http://academicpages.github.io/files/paper1.pdf'
citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, 
the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to
the sparse attention mechanism. Theoretically, our key contribution is a principled
derivation of a closed-form sparse Hopfield energy using the convex conjugate of
the sparse entropic regularizer. Building upon this, we derive the sparse memory
retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a
sparsity-dependent memory retrieval error bound which is provably tighter than
its dense analog. The conditions for the benefits of sparsity to arise are therefore
identified and discussed. In addition, we show that the sparse modern Hopfield
model maintains the robust theoretical properties of its dense counterpart, including
rapid fixed point convergence and exponential memory capacity. Empirically, we
use both synthetic and real-world datasets to demonstrate that the sparse Hopfield
model outperforms its dense counterpart in many situations.

[Download paper here](http://academicpages.github.io/files/paper1.pdf)

